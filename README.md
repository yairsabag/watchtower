# ğŸ—¼ Watchtower

**Runtime loop detection for multi-agent AI teams.**

Watchtower monitors your LangGraph multi-agent system in real-time and catches infinite loops before they burn through your token budget.

One line to integrate. Zero config. Works today.

## The Problem

Multi-agent systems fail silently. A research agent keeps searching because it's "never satisfied." An analysis agent keeps requesting more data. Two agents ping-pong forever. You don't notice until the bill arrives.

Existing observability tools (Langfuse, LangSmith, Helicone) track individual LLM calls â€” cost, latency, tokens. **None of them understand team dynamics.** They see trees, not the forest.

## The Solution

```python
from watchtower import watch

graph = watch(your_langgraph)       # â† one line
result = graph.invoke(your_input)   # â† same as before
```

Watchtower computes a **Loop Score** (0.0 â†’ 1.0) in real-time from three signals:

| Signal | What it detects |
|--------|----------------|
| **Node Repetition** | Same agents running over and over |
| **Sequence Cycles** | Repeating patterns like Aâ†’Bâ†’Aâ†’Bâ†’Aâ†’B |
| **Tool Call Repeats** | Same tool called with same parameters |

When the score crosses a threshold â€” one alert, execution stops, money saved.

## Try It (30 seconds)

```bash
git clone https://github.com/yairsabag/watchtower.git
cd watchtower
pip install langgraph langchain-core
python -m demo.loop_demo
```

What you'll see:

```
ğŸ—¼ watchtower v0.1.0
monitoring: LangGraph
metrics:    Loop Score

[watchtower] step  1 research_agent â†’ web_search(query=AI agent market analysis) âœ“
[watchtower] step  2 analysis_agent â†’ analyze âœ“
[watchtower] step  3 research_agent â†’ web_search(query=AI agent market analysis) ~ score:0.34
[watchtower] step  4 analysis_agent â†’ analyze âš¡ repeat score:0.77
[watchtower] step  5 research_agent â†’ web_search(query=AI agent market analysis) âš¡ repeat score:0.74
[watchtower] step  6 analysis_agent â†’ analyze âš¡ repeat score:0.81

  âš ï¸  LOOP DETECTED
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Score:     0.81  ğŸ”´
  Pattern:   research_agentâ†’analysis_agent  (Ã—3)
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  â±  Caught at step 6 â€” without detection this loop would continue indefinitely.

  watchtower summary
  Steps monitored:  6
  Loop detected:    yes
  Peak loop score:  0.81

  âœ“ Watchtower caught the loop and stopped execution.
```

## Use on Your Own Graph

```python
from watchtower import watch

# Your existing code
graph = build_my_graph().compile()

# Add Watchtower
monitored = watch(graph)
result = monitored.invoke(my_input)
```

### Options

```python
# Adjust sensitivity
monitored = watch(graph, threshold=0.8)

# Stop execution on loop detection
from watchtower import watch, StopMonitoring

def on_loop(result):
    print(f"Loop! {result.pattern} (Ã—{result.repeat_count})")
    raise StopMonitoring()

monitored = watch(graph, on_loop=on_loop)

# Silent mode â€” no terminal output, just callbacks
monitored = watch(graph, silent=True, on_loop=my_callback)

# Access results after run
print(monitored.total_steps)
print(monitored.max_score)
print(monitored.alerts)
```

## How It Works

Watchtower wraps your compiled LangGraph and listens to every node execution via `stream()`. It doesn't modify behavior â€” it only observes. On each step, it updates a sliding window and computes Loop Score from the three signals. If the score crosses the threshold and the pattern has repeated 3+ times across 5+ steps, it fires one alert.

No ML models. No embeddings. No external API calls. Pure pattern matching that adds near-zero overhead.

## Roadmap

- [x] Loop Score (node repetition + sequence cycles + tool call repeats)
- [ ] Tool Thrash Index (wasted tool calls that don't advance the task)
- [ ] CrewAI adapter
- [ ] AutoGen adapter
- [ ] Webhook / JSON export
- [ ] Dashboard

## License

MIT â€” use it however you want.

---

**Built because multi-agent teams deserve the same observability that infrastructure has had for decades.**
